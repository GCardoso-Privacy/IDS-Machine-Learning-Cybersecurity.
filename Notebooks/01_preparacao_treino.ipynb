{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b91d3f8-a304-4b49-8707-5460b0b6008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> INICIANDO CRIA√á√ÉO DO DATASET DE TREINO (Amostra: 10.0%) <<<\n",
      "üìÇ Encontrados 8 arquivos em E:\\Estudos_Cybersecurity\\Datasets_Cybersecurity\\CICIDS2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Encontrados 18 arquivos em E:\\Estudos_Cybersecurity\\Datasets_Cybersecurity\\CICDDoS2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados:  11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                         | 2/18 [00:28<04:22, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao ler TFTP.parquet: malloc of size 1048576 failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                      | 3/18 [00:59<05:45, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao ler Syn.parquet: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                  | 4/18 [01:01<03:29, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao ler DrDoS_UDP.parquet: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 6/18 [01:55<04:40, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro ao ler DrDoS_SNMP.parquet: malloc of size 1073741824 failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Amostrando Dados: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [07:16<00:00, 24.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Concatenando dados (Unindo 2017 + 2019)...\n",
      "üìä Dimens√µes Finais: 4326968 linhas x 88 colunas\n",
      "üíæ Salvando em: E:\\Estudos_Cybersecurity\\Datasets_Cybersecurity\\dataset_treino_final.parquet ...\n",
      "\n",
      "‚úÖ SUCESSO! Dataset de treino criado.\n",
      "Pr√≥ximo passo: An√°lise Explorat√≥ria e Treinamento do Modelo.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Configura√ß√£o para ignorar avisos irrelevantes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURA√á√ïES\n",
    "# =============================================================================\n",
    "BASE_DIR = r\"E:\\Estudos_Cybersecurity\\Datasets_Cybersecurity\"\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, \"dataset_treino_final.parquet\")\n",
    "\n",
    "# Fra√ß√£o de amostragem (0.1 = 10% dos dados)\n",
    "# Isso √© suficiente para criar um modelo robusto sem travar a mem√≥ria RAM\n",
    "SAMPLE_RATE = 0.10 \n",
    "\n",
    "# Mapeamento para corrigir nomes de colunas diferentes entre 2017 e 2019\n",
    "column_mapping = {\n",
    "    \"Flow ID\": \"Flow_ID\",\n",
    "    \"Source IP\": \"Source_IP\",\n",
    "    \"Source Port\": \"Source_Port\",\n",
    "    \"Destination IP\": \"Destination_IP\",\n",
    "    \"Destination Port\": \"Destination_Port\",\n",
    "    \"Protocol\": \"Protocol\",\n",
    "    \"Timestamp\": \"Timestamp\",\n",
    "    # Adicione outros se necess√°rio, mas esses s√£o os principais identificadores\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FUN√á√ÉO DE LEITURA E AMOSTRAGEM\n",
    "# =============================================================================\n",
    "def carregar_e_amostrar(diretorio_raiz):\n",
    "    dfs = []\n",
    "    \n",
    "    # Busca recursiva por todos os arquivos .parquet\n",
    "    arquivos = glob.glob(os.path.join(diretorio_raiz, \"**\", \"*.parquet\"), recursive=True)\n",
    "    \n",
    "    # Remove o arquivo final se ele j√° existir na lista (evita ler o pr√≥prio output)\n",
    "    arquivos = [f for f in arquivos if \"dataset_treino_final\" not in f]\n",
    "    \n",
    "    print(f\"üìÇ Encontrados {len(arquivos)} arquivos em {diretorio_raiz}\")\n",
    "\n",
    "    for arquivo in tqdm(arquivos, desc=\"Amostrando Dados\"):\n",
    "        try:\n",
    "            # L√™ o arquivo Parquet (muito r√°pido!)\n",
    "            df_temp = pd.read_parquet(arquivo)\n",
    "            \n",
    "            # 1. Renomear colunas para padr√£o √∫nico\n",
    "            df_temp.rename(columns=column_mapping, inplace=True)\n",
    "            \n",
    "            # 2. Remover espa√ßos dos nomes das colunas (ex: \" Label\" -> \"Label\")\n",
    "            df_temp.columns = df_temp.columns.str.strip()\n",
    "            \n",
    "            # 3. Tratamento b√°sico de Label (padronizar mai√∫sculas/min√∫sculas)\n",
    "            if 'Label' in df_temp.columns:\n",
    "                df_temp['Label'] = df_temp['Label'].astype(str).str.strip()\n",
    "            \n",
    "            # 4. AMOSTRAGEM ALEAT√ìRIA\n",
    "            # Pegamos apenas 10% deste arquivo de forma aleat√≥ria\n",
    "            df_sample = df_temp.sample(frac=SAMPLE_RATE, random_state=42)\n",
    "            \n",
    "            # Adiciona colunas identificadoras (opcional, bom para rastreio)\n",
    "            df_sample['origem_arquivo'] = os.path.basename(arquivo)\n",
    "            \n",
    "            dfs.append(df_sample)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao ler {os.path.basename(arquivo)}: {e}\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "# =============================================================================\n",
    "# 3. EXECU√á√ÉO\n",
    "# =============================================================================\n",
    "print(f\">>> INICIANDO CRIA√á√ÉO DO DATASET DE TREINO (Amostra: {SAMPLE_RATE*100}%) <<<\")\n",
    "\n",
    "# Carrega listas de DataFrames\n",
    "dados_2017 = carregar_e_amostrar(os.path.join(BASE_DIR, \"CICIDS2017\"))\n",
    "dados_2019 = carregar_e_amostrar(os.path.join(BASE_DIR, \"CICDDoS2019\")) # Pega Treino e Teste\n",
    "\n",
    "# Junta tudo em uma lista √∫nica\n",
    "todos_dfs = dados_2017 + dados_2019\n",
    "\n",
    "if todos_dfs:\n",
    "    print(\"\\nüîÑ Concatenando dados (Unindo 2017 + 2019)...\")\n",
    "    # Concatena ignorando √≠ndices antigos (reset_index)\n",
    "    df_final = pd.concat(todos_dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"üìä Dimens√µes Finais: {df_final.shape[0]} linhas x {df_final.shape[1]} colunas\")\n",
    "    \n",
    "    # Salva o arquivo final consolidado\n",
    "    print(f\"üíæ Salvando em: {OUTPUT_FILE} ...\")\n",
    "    df_final.to_parquet(OUTPUT_FILE, compression='snappy')\n",
    "    \n",
    "    print(\"\\n‚úÖ SUCESSO! Dataset de treino criado.\")\n",
    "    print(\"Pr√≥ximo passo: An√°lise Explorat√≥ria e Treinamento do Modelo.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå ERRO: Nenhum dado foi carregado. Verifique os caminhos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e7cfd-b8c9-47db-b4a9-188fa55545d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
